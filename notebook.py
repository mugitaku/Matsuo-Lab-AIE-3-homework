# -*- coding: utf-8 -*-
"""ai_engineering_03_T4.ipynb のコピー

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y1ei2OYcuaes-DcVdYqm9e2Kfm7Fg81t

### 演習環境の準備
"""

!pip install --upgrade transformers
!pip install google-colab-selenium
!pip install bitsandbytes

# 演習用のコンテンツを取得
!git clone https://github.com/mugitaku/Matsuo-Lab-AIE-3-homework

from huggingface_hub import notebook_login

notebook_login()

# CUDAが利用可能ならGPUを、それ以外ならCPUをデバイスとして設定
import torch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

import random
random.seed(0)

# モデル(Gemma2)の読み込み

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_name = "google/gemma-2-2b-jpn-it"
tokenizer = AutoTokenizer.from_pretrained(model_name)

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=False,
)

model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map="auto",
            quantization_config=bnb_config,
            torch_dtype=torch.bfloat16,
        )

"""# 1. ベースラインモデル評価
まずはベースモデルがどの程度知識を持っているか確かめる
"""

messages = [
    {"role": "user", "content": "X氏「私は雨の日が嫌いだ。」Y氏「もし雨が降らなかったら干ばつで農作物は枯れ、ダムは枯渇し我々はみな餓死することになるが、それでもX氏は雨など無くなったほうが良いと言うのであろうか。」Y氏の主張は次のうちどれに該当するか。「A.衆人に訴える論証」「B.権威に訴える論証」「C.ストローマン」「D.いずれにも該当しない」"},
]
input_ids = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    return_tensors="pt"
).to(model.device)

terminators = [
    tokenizer.eos_token_id,
    tokenizer.convert_tokens_to_ids("<|eot_id|>")
]

outputs = model.generate(
    input_ids,
    max_new_tokens=256,
    eos_token_id=terminators,
    do_sample=False,
    # temperature=0.6, # If do_sample=True
    # top_p=0.9,  # If do_sample=True
)

response = outputs[0][input_ids.shape[-1]:]
print(tokenizer.decode(response, skip_special_tokens=True))

"""# 2. 文字起こしデータの活用"""

from sentence_transformers import SentenceTransformer

emb_model = SentenceTransformer("infly/inf-retriever-v1-1.5b", trust_remote_code=True)
# In case you want to reduce the maximum length:
emb_model.max_seq_length = 4096

!rm -rf Matsuo-Lab-AIE-3-homework  # フォルダ削除（中のデータに注意）
!git clone https://github.com/mugitaku/Matsuo-Lab-AIE-3-homework

with open("/content/Matsuo-Lab-AIE-3-homework/Argumentum-ad-populum.txt", "r") as f:
  raw_writedown = f.read()

# ドキュメントを用意する。
documents = [text.strip() for text in raw_writedown.split("\n")]
#documents = raw_writedown
print(documents)
print("ドキュメントサイズ: ", len(documents))
print("ドキュメントの例: \n", documents[0])

# Retrievalの実行
question = "X氏「私は雨の日が嫌いだ。」Y氏「もし雨が降らなかったら干ばつで農作物は枯れ、ダムは枯渇し我々はみな餓死することになるが、それでもX氏は雨など無くなったほうが良いと言うのであろうか。」Y氏の主張は次のうちどれに該当するか。「A.衆人に訴える論証」「B.権威に訴える論証」「C.ストローマン」「D.いずれにも該当しない」"

query_embeddings = emb_model.encode([question], prompt_name="query")
document_embeddings = emb_model.encode(documents)

# 各ドキュメントの類似度スコア
scores = (query_embeddings @ document_embeddings.T) * 100
print(scores.tolist())

topk = 5
for i, index in enumerate(scores.argsort()[0][::-1][:topk]):
  print(f"取得したドキュメント{i+1}: (Score: {scores[0][index]})")
  print(documents[index], "\n\n")

references = "\n".join(["* " + documents[i] for i in scores.argsort()[0][::-1][:topk]])
messages = [
    {"role": "user", "content": f"[参考資料]\n{references}\n\n[質問]X氏「私は雨の日が嫌いだ。」Y氏「もし雨が降らなかったら干ばつで農作物は枯れ、ダムは枯渇し我々はみな餓死することになるが、それでもX氏は雨など無くなったほうが良いと言うのであろうか。」Y氏の主張は次のうちどれに該当するか。「A.衆人に訴える論証」「B.権威に訴える論証」「C.ストローマン」「D.いずれにも該当しない」"},
]
input_ids = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    return_tensors="pt"
).to(model.device)

terminators = [
    tokenizer.eos_token_id,
    tokenizer.convert_tokens_to_ids("<|eot_id|>")
]

outputs = model.generate(
    input_ids,
    max_new_tokens=256,
    eos_token_id=terminators,
    do_sample=False,
    # temperature=0.6, # If do_sample=True
    # top_p=0.9,  # If do_sample=True
)

response = outputs[0][input_ids.shape[-1]:]
print(tokenizer.decode(response, skip_special_tokens=True))